# 词频分析 & 词云生成
## 背景
目前手上有了几篇关于某个领域的科研论文, 为了尽快了解这个方向的研究热点, 其中一种比较快速的方法是: 首先根据少数几个关键字, 去相应的数据库(我用的比较多的是 PubMed)进行关键字检索, 设置好检索范围(很重要, 可以过滤掉很多不相关或不重要的论文), 拿到符合要求的论文(一般就几十篇), 并对这些论文进行词频分析, 找出其中有实际意义的高频词, 这样就可以确定我们的研究重点了. 
## 读取 PDF 文件
论文下载下来以后, 第一步当然就是读取了. 我这里是下载的 PDF 文件, 读取方法比较特别, 可以自己利用 `pdfminer`等第三方 Python 库自己写脚本读取, 也可以利用`pdftotext`等命令行工具来转换格式([上一篇文章](https://github.com/focusxyhoo/my-notebook/blob/master/Tools/PDF%20%E8%BD%AC%20TXT%20%E6%80%BB%E7%BB%93.md)有介绍过). 方法有很多, 自己选择就行. 
## 词频分析
首先对不同的语言, 词频分析的工作流程是不太一样的. 我这里处理的是英文, 相对中文来说简单一些. 词频分析的工作原理并不复杂, 无非就是统计每个词的出现次数(中文需要加一步**分词**). 这个过程中麻烦的是对于词形变化(尤其是英文)的处理. 

这里不考虑太过复杂的情况, 只简单列一下代码, 以及几个工具的使用. 

```Python
import os
from os import path
from collections import Counter

def get_text(file):
    text = open(file, "r").read()
    text = text.lower()  # 将所有单词转换为小写去掉大小写的干扰
    # 去掉所有的特殊符号
    for ch in '`!@#~$%^&*()_+-=*/{}[];,./?<>':
        text = text.replace(ch, " ")  # 将特殊符号替换成空格 即去掉
    return text

def get_counter(directory, stop_words):
    files = os.listdir(directory)
    counter = Counter()
    for file in files:
        full_path = path.join(directory, file)
        text = get_text(full_path)
        words = text.split()
        for word in words:
            if len(word) > 1 and word not in stop_words and not word.isdigit():
                counter[word] += 1
    return counter
```
上面的例子给出了一个简单的示范: 读取文件, 并统计每个单词的出现次数. 当然, 也可以通过其他 Python 第三方库(如`NLTK`)来完成这个过程. 当对这些库比较熟悉时, 会很方便, 写出来的代码也会更加简洁. 

```python
from nltk import word_tokenize
raw = get_text(file)
tokens = word_tokenize(raw)
```
按照官方文档的解释, tokenization 的意思是, 将文本字符串打断成单词和标点符号的过程. 这个过程产生的结果正是我们需要的, 即单词和标点符号的列表(**注意**: 在`get_text()`方法中, 我们已经过滤掉了标点符号). 这样, 在 tokenization 之后, 可以直接对`tokens`进行处理. 具体处理方法可以去查阅官方文档(参考资料 3).
## 词云生成
网络上已经有了许多现成的词云生成工具. 只需要将文本复制到工具中, 即可自动按照所定制的样式生成词云图. 这种方法非常简单快速, 推荐优先使用.

同样的, 利用 Python 第三方库`wordcloud`也可以自定义生成词云图. 参考资料 2 中是关于`wordcloud`的一篇博文, 可以详细读一读.

使用非常简单, 具体代码如下: 
```python
def word_cloud(img_path, counter):
    # 制作云图
    wordcount = WordCloud(background_color='white',
                          max_font_size=80, scale=5,
                          max_words=150,
                          min_font_size=5
                          )
    wordcount = wordcount.generate_from_frequencies(counter)
    wordcount.to_file(img_path)
```
使用过程中遇到的几个问题:
1. 对于英文不需要设置字体路径，但是中文需要，否则无法显示。设置方法为 font_path="/Users/huxiaoyang/Downloads/SimHei.ttf";
2. 数据可以从词频中获得, 也可以从文本中获得, 如 wordcount.generate_from_text(text);
3. 按照默认样式生成的词云图一般比较模糊, 可以通过调大`scale`参数来解决. 
4. WordCloud设置参数比较多, 建议都看一下, 这样可以快速调整词云图的样式, 以满足要求.

关于`wordcloud`的资料也有很多, 参考资料 4 中介绍了详细的使用方法.

## 参考资料
1. [Natural Language Toolkit](https://www.nltk.org/)
2. [A Wordcloud in Python](http://peekaboo-vision.blogspot.com/2012/11/a-wordcloud-in-python.html)
3. [wordcloud GitHub](https://github.com/amueller/word_cloud)
4. [Python 可视化(4)：WordCloud 中英文词云图绘制方法汇总](https://www.makcyun.top/Python_visualization04.html)
