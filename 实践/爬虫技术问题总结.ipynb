{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础内容\n",
    "## 了解网页结构\n",
    "### 什么是HTML\n",
    "HTML 是一种浏览器(Chrome, Safari, IE, Firefox等)看得懂的语言, 浏览器能将这种语言转换成我们用肉眼看到的网页。我们要做的就是从其中爬取出我们需要的信息。\n",
    "### 还有CSS和JavaScript\n",
    "构成多彩、多功能网页组件。\n",
    "### 网页基本组成\n",
    "在HTML中，有许多tag，其中包裹着基本上所有的实体内容。tag主要分成两个部分，`header`和`body`。在`header`中，存放一些网页的元信息，如`title`，这些信息不会被浏览器显示出来。而在`body`中存放的才是我们能够在浏览器上看到的内容，比如：`<h1></h1>`就是主标题，`<h></h>`就是一个段落，`<a></a>`就是链接。\n",
    "\n",
    "**爬虫想要做的就是根据这些 tag 来找到合适的信息。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬网页\n",
    "利用Python爬取网页的基本信息，并打印出这个网页HTML的source code："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:44:51.852343Z",
     "start_time": "2019-02-28T08:44:51.105186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"cn\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Scraping tutorial 1 | 莫烦Python</title>\n",
      "\t<link rel=\"icon\" href=\"https://morvanzhou.github.io/static/img/description/tab_icon.png\">\n",
      "</head>\n",
      "<body>\n",
      "\t<h1>爬虫测试1</h1>\n",
      "\t<p>\n",
      "\t\t这是一个在 <a href=\"https://morvanzhou.github.io/\">莫烦Python</a>\n",
      "\t\t<a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t</p>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# url = 'https://www.baidu.com/'\n",
    "url = \"https://morvanzhou.github.io/static/scraping/basic-structure.html\"\n",
    "# 网页中存在中文，需要在read()完之后使用decode()来进行转码\n",
    "html = urlopen(url).read().decode('utf-8')\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过分析网页HTML可以知道，合理地利用tag的name属性可以快速提取到我们需要的信息。\n",
    "## 匹配网页内容\n",
    "### 利用正则表达式\n",
    "简单的网页匹配通过正则表达式就可以搞定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:44:51.860615Z",
     "start_time": "2019-02-28T08:44:51.855575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page title is:  Scraping tutorial 1 | 莫烦Python\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 获取标题的内容\n",
    "res = re.findall(r\"<title>(.+?)</title>\", html)\n",
    "print(\"\\nPage title is: \", res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:44:51.867004Z",
     "start_time": "2019-02-28T08:44:51.862914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page paragraph is:  \n",
      "\t\t这是一个在 <a href=\"https://morvanzhou.github.io/\">莫烦Python</a>\n",
      "\t\t<a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "# 获取章节的内容\n",
    "res = re.findall(r\"<p>(.*?)</p>\", html, flags=re.DOTALL)\n",
    "print(\"\\nPage paragraph is: \", res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际应用中比较常用的时：找出一个网页上的所有链接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:44:51.876700Z",
     "start_time": "2019-02-28T08:44:51.871116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All links:  ['https://morvanzhou.github.io/static/img/description/tab_icon.png', 'https://morvanzhou.github.io/', 'https://morvanzhou.github.io/tutorials/data-manipulation/scraping/']\n"
     ]
    }
   ],
   "source": [
    "res = re.findall(r'href=\"(.*?)\"', html)\n",
    "print(\"\\nAll links: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "复杂一点的网页就需要其他工具的帮助。首先，我们先梳理一下爬网页的流程：\n",
    "+ 找到要爬网页的网址\n",
    "+ 使用Python登录上这个网址（`urlopen()`等）\n",
    "+ 读取网页信息（`read()等`）\n",
    "+ 将读取的信息放入BeautifulSoup\n",
    "+ 使用BeautifulSoup选取tag信息等（代替正则表达式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:44:51.979374Z",
     "start_time": "2019-02-28T08:44:51.881186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>爬虫测试1</h1>\n",
      "\n",
      " <p>\n",
      "\t\t这是一个在 <a href=\"https://morvanzhou.github.io/\">莫烦Python</a>\n",
      "<a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t</p>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# 推荐使用lxml解析器\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "print(soup.h1)\n",
    "print('\\n',soup.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当网页中有多个相同的tag时，可以通过`find_all()`找到所有的选项。又注意到，链接其实是存在于`<a href=\"link\">`里面，因此可以看作是tag`<a>`的一个属性，可以像字典一样进行读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:44:51.984981Z",
     "start_time": "2019-02-28T08:44:51.981257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['https://morvanzhou.github.io/', 'https://morvanzhou.github.io/tutorials/data-manipulation/scraping/']\n"
     ]
    }
   ],
   "source": [
    "all_href = soup.find_all(\"a\")\n",
    "all_href = [l['href'] for l in all_href]\n",
    "print('\\n', all_href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bs4加上CSS\n",
    "CSS的主要作用就是装饰HTML，使页面内容更加丰富。\n",
    "##### CSS的Class\n",
    "在爬虫中，我们主要了解CSS的Class，CSS在装饰每一个网页部件的时候，都会给它一个名字，而且一个类型的部件，名字都可以一样。利用这一特性，我们可以快速找到我们想要的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:44:53.343708Z",
     "start_time": "2019-02-28T08:44:51.987034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"cn\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>爬虫练习 列表 class | 莫烦 Python</title>\n",
      "\t<style>\n",
      "\t.jan {\n",
      "\t\tbackground-color: yellow;\n",
      "\t}\n",
      "\t.feb {\n",
      "\t\tfont-size: 25px;\n",
      "\t}\n",
      "\t.month {\n",
      "\t\tcolor: red;\n",
      "\t}\n",
      "\t</style>\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "\n",
      "<h1>列表 爬虫练习</h1>\n",
      "\n",
      "<p>这是一个在 <a href=\"https://morvanzhou.github.io/\" >莫烦 Python</a> 的 <a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\" >爬虫教程</a>\n",
      "\t里无敌简单的网页, 所有的 code 让你一目了然, 清晰无比.</p>\n",
      "\n",
      "<ul>\n",
      "\t<li class=\"month\">一月</li>\n",
      "\t<ul class=\"jan\">\n",
      "\t\t<li>一月一号</li>\n",
      "\t\t<li>一月二号</li>\n",
      "\t\t<li>一月三号</li>\n",
      "\t</ul>\n",
      "\t<li class=\"feb month\">二月</li>\n",
      "\t<li class=\"month\">三月</li>\n",
      "\t<li class=\"month\">四月</li>\n",
      "\t<li class=\"month\">五月</li>\n",
      "</ul>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://morvanzhou.github.io/static/scraping/list.html\"\n",
    "html = urlopen(url).read().decode(\"utf-8\")\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 按Class匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:44:53.353469Z",
     "start_time": "2019-02-28T08:44:53.345617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一月\n",
      "二月\n",
      "三月\n",
      "四月\n",
      "五月\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "month = soup.find_all('li', {\"class\": \"month\"})\n",
    "for m in month:\n",
    "    print(m.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bs4加上re\n",
    "例如：找出一个网页中的全部jpg格式图片的链接。通过观察可以知道，图片链接都藏在`<img src=\"link\">`中，那么利用BeautifulSoup可以很快速找到，再通过正则表达式可以快速过滤掉其他格式的图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:50:17.580960Z",
     "start_time": "2019-02-28T08:50:16.760467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://morvanzhou.github.io/static/img/course_cover/tf.jpg\n",
      "https://morvanzhou.github.io/static/img/course_cover/rl.jpg\n",
      "https://morvanzhou.github.io/static/img/course_cover/scraping.jpg\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url = \"https://morvanzhou.github.io/static/scraping/table.html\"\n",
    "html = urlopen(url).read().decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "img_links = soup.find_all(\"img\", {\"src\": re.compile('.*?\\.jpg')})\n",
    "for link in img_links:\n",
    "    print(link['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何添加代理\n",
    "## 使用proxy的步骤\n",
    "1. 设置代理地址：`proxy = {'http': ''}`\n",
    "2. 创建ProxyHeader：`proxyHeader = request.ProxyHandler(proxy)`\n",
    "3. 创建Opener：`opener = request.build_opener(proxyHeader)`\n",
    "4. 安装Opener：`request.install_opener(opener)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T03:07:19.652624Z",
     "start_time": "2019-03-01T03:06:48.641789Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "# 设置代理地址\n",
    "proxy = {'http':'52.187.162.198:3128'}\n",
    "# 创建ProxyHeader\n",
    "proxyHeader = request.ProxyHandler(proxy)\n",
    "# 创建Opener\n",
    "opener = request.build_opener(proxyHeader)\n",
    "# 安装Opener\n",
    "request.install_opener(opener)\n",
    "# 然后剩下的就跟正常使用差不多，只不过此时的request已经是绑定了代理之后的request\n",
    "url = 'https://www.ncbi.nlm.nih.gov/pmc'\n",
    "req = request.Request(url)\n",
    "response = request.urlopen(req)\n",
    "# print(response.read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小例子: 爬网页\n",
    "我们去爬取PMC数据库中的内容。观察网页源代码可以看到，每页20条检索结果，均藏在`<div class=\"rprt\">`下的`<div class=\"rslt\">`中，其中，`<div class=\"title\">`中的tag`<a>`存放着搜索结果，即文献名，其属性`ref=\"index\"`存放对应的链接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-28T09:42:27.898Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import random\n",
    "\n",
    "base_url = \"https://www.ncbi.nlm.nih.gov/pmc\"\n",
    "his = [\"/?term=diabetes\"]\n",
    "\n",
    "url = base_url + his[-1]\n",
    "\n",
    "html = urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "result = soup.find_all('div', {\"class\": \"rslt\"})\n",
    "\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用requests\n",
    "注意，获取网页的方式是不一样的，常用的方式有GET和POST，一般情况下，前者注意用来打开网页，不往服务器传数据，而后者常用于登录账号，搜索内容，上传文件/图片，往服务器传数据等。\n",
    "## requests get请求\n",
    "通过观察地址栏的规律，可以很快利用requests可以模拟网页上的搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T03:44:28.373516Z",
     "start_time": "2019-03-01T03:44:27.626111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/s?wd=Python%E6%96%87%E6%A1%A3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import webbrowser\n",
    "# 以字典存储搜索关键字\n",
    "param = {\"wd\": \"Python文档\"}\n",
    "r = requests.get(\"http://www.baidu.com/s\", params=param)\n",
    "print(r.url)\n",
    "# 利用webbrowser模块打开浏览器观察搜索结果\n",
    "webbrowser.open(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests post请求\n",
    "注意POST与GET的区别，POST以表单的形式将数据加密提交给网页，而GET提交的信息通常能反应在URL上。通常我们需要注意三个内容：\n",
    "+ Request URL: POST要使用的URL, 注意不是我们填表时的URL, \n",
    "+ Request Method: POST,\n",
    "+ Form Data: 即我们提交的内容."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T03:44:34.493439Z",
     "start_time": "2019-03-01T03:44:32.975571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there,  !\n"
     ]
    }
   ],
   "source": [
    "data = {'firstname': 'Focusxy', 'lastname': 'Hu'}\n",
    "r = requests.post('http://pythonscraping.com/files/processing.php', data=data)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上传照片\n",
    "需要注意: \n",
    "+ 观察URL的变化, 传照片前后一般是有变化的,\n",
    "+ 观察提交按钮的name, 加入Python字典的key,\n",
    "+ 确定python字典的值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T03:44:38.962862Z",
     "start_time": "2019-03-01T03:44:35.650726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploads/\n",
      "Sorry, there was an error uploading your file.\n"
     ]
    }
   ],
   "source": [
    "file = {'uploadFile': open('./steel.jpg', 'rb')}\n",
    "r = requests.post('http://pythonscraping.com/files/processing2.php', files=file)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模拟登录\n",
    "重要的三点:\n",
    "+ Request URL,\n",
    "+ Form Data中的用户名和密码,\n",
    "+ Cookies. Cookies中存储的是用户的登录信息.\n",
    "\n",
    "通过`requests.post + payload`将用户信息发给服务器, 返回的对象中会有生成的cookies信息, 我们可以将cookies传入到get请求, 这样就可以以登录的名义来访问get页面了."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:32:45.902063Z",
     "start_time": "2019-03-01T04:32:43.625521Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c0ee94d332c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Morvan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'password'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'password'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://pythonscraping.com/pages/cookies/welcome.php'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://pythonscraping.com/pages/cookies/profile.php'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "payload = {'username': 'Morvan', 'password': 'password'}\n",
    "r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)\n",
    "print(r.cookies.get_dict())\n",
    "\n",
    "r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', cookies=r.cookies)\n",
    "# print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Session登录\n",
    "使用session可以不用反复向get中传入cookies, 因为session内部已经有了之前的cookies了."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:34:37.160997Z",
     "start_time": "2019-03-01T04:34:35.187043Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-115660f0d7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Morvan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'password'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'password'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://pythonscraping.com/pages/cookies/welcome.php'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "payload = {'username': 'Morvan', 'password': 'password'}\n",
    "r = session.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)\n",
    "print(r.cookies.get_dict())\n",
    "\n",
    "r = session.get(\"http://pythonscraping.com/pages/cookies/profile.php\")\n",
    "# print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下载文件\n",
    "需要观察网站源码找出文件的下载地址, 注意是完整的地址. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:51:57.550152Z",
     "start_time": "2019-03-01T04:51:57.546677Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# 建立一个文件夹专门用来存储图片\n",
    "os.makedirs('./img/', exist_ok=True)\n",
    "\n",
    "IMAGE_URL = \"https://morvanzhou.github.io/static/img/description/learning_step_flowchart.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用urlretrieve\n",
    "urllib模块中提供了一个使用非常简单的下载功能urlretrieve, 传入下载地址`IMAGE_URL`和要存放的文件路径就可以了."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:54:25.363816Z",
     "start_time": "2019-03-01T04:54:23.447018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./img/image1.png', <http.client.HTTPMessage at 0x112972f60>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve(IMAGE_URL, './img/image1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用request\n",
    "使用requests模块下载文件, 代码稍长一些, 但是可以更有效率地下载大文件."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:57:06.619751Z",
     "start_time": "2019-03-01T04:56:52.145278Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(IMAGE_URL)\n",
    "with open('./img/image2.png', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`r.iter_content(chunk_size)`可以控制chunk的大小, 使文件一个chunk一个chunk的下载, 不必全部下载完全才能保存."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T05:00:29.978644Z",
     "start_time": "2019-03-01T05:00:28.664624Z"
    }
   },
   "outputs": [],
   "source": [
    "# stream loading\n",
    "r = requests.get(IMAGE_URL, stream=True)\n",
    "with open('./img/image3.png', 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=32):\n",
    "        f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T05:05:09.208335Z",
     "start_time": "2019-03-01T05:05:09.205810Z"
    }
   },
   "source": [
    "# 小例子: 下载\n",
    "待完善。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 效率问题\n",
    "待完善。\n",
    "## 多线程\n",
    "采用分布式爬虫可以大大节省程序的运行时间. 根据爬虫的流程, 可以定义两个功能, 一个是用来爬取网页的crawl, 一个是解析网页的parse. 通过这两部分的合作, 加上剔除重复的网址, 基本上可以快速爬取完我们需要的内容."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T05:41:33.739112Z",
     "start_time": "2019-03-01T05:41:33.732498Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-c557e01133d9>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c557e01133d9>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    urls = soup.find_all(‘a’, {\"href\": \"re.compile('')\"})\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "from urllib.request import urlopen, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "base_url = 'https://morvanzhou.github.io/'\n",
    "\n",
    "def crawl(url):\n",
    "    response = urlopen(url)\n",
    "    return response.read().decode() # 网页中有中文内容要加decode()\n",
    "\n",
    "def parse(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    urls = soup.find_all('a', {\"href\": re.compile('')})\n",
    "    title = soup.find()\n",
    "    \n",
    "    # 利用Python自带的set去重\n",
    "    page_urls = set(urljoin(base_url, url['href']) for url in urls)\n",
    "    url = soup.find('meta', {'property': ''})['content']\n",
    "    \n",
    "    return title, page_urls, url\n",
    "\n",
    "# 定义两个set, 用来搜集爬过的和还没爬过的\n",
    "unseen = set([base_url,])\n",
    "seen = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajax动态网页\n",
    "Ajax可以在不重新加载整个网页的基础上，对网页的部分进行更新。这样看上去网页的URL没有变化，那么如何爬取更新的内容呢？\n",
    "\n",
    "（**注意**：网页URL没有变化不一定就是采用了ajax技术。采用ajax技术的网页，在跳转下一页或者加载更多内容时，地址栏没有刷新。）\n",
    "\n",
    "Ajax技术的核心是`XMLHttpRequest`对象（简称XHR），可以通过使用XHR对象获取到服务器的数据，然后再通过DOM将数据插入到页面中呈现。虽然名字中包含XML，但Ajax通讯与数据格式无关，所以我们的数据格式可以是XML或JSON等格式。\n",
    "## 如何爬取ajax动态加载的网页\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
